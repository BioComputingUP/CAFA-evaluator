{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "plt.set_loglevel(\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "# df_file = \"results/evaluation_all.tsv\"\n",
    "# out_folder = \"results\"\n",
    "\n",
    "df_file = \"/home/damiano/Projects/CAFA-evaluator_data/pkg_example/results/evaluation_all.tsv\"\n",
    "out_folder = \"/home/damiano/Projects/CAFA-evaluator_data/pkg_example/results\"\n",
    "\n",
    "# Set to None if you don't want to use it. Results will not be grouped/filtered by team\n",
    "names_file = None\n",
    "\n",
    "# Cumulate the last column of the cols variable, e.g. \"pr\" --> precision, so that the curves are monotonic as in CAFA\n",
    "cumulate = True\n",
    "\n",
    "# Methods with coverage below this threshold will not be plotted\n",
    "coverage_threshold = 0.3\n",
    "\n",
    "# Select a metric\n",
    "# metric, cols = ('f', ['rc', 'pr'])\n",
    "# metric, cols =  ('f_w', ['rc_w', 'pr_w'])\n",
    "metric, cols =  ('f_micro', ['rc_micro', 'pr_micro'])\n",
    "# metric, cols =  ('f_micro_w', ['rc_micro_w', 'pr_micro_w'])\n",
    "# metric, cols = ('s_w', ['ru_w', 'mi_w'])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Map column short names to full names (for axis labels)\n",
    "axis_title = {'pr': 'Precision', 'rc': 'Recall', 'f': 'F-score', 'pr_w': 'Weighted Precision', 'rc_w': 'Weighted Recall', 'f_w': 'Weighted F-score', 'mi': 'Misinformation', 'ru': 'Remaining Uncertainty', 's': 'S-score', 'pr_micro': 'Precision (Micro)', 'rc_micro': 'Recall (Micro)', 'f_micro': 'F-score (Micro)', 'pr_micro_w': 'Weighted Precision (Micro)', 'rc_micro_w': 'Weighted Recall (Micro)', 'f_micro_w': 'Weighted F-score (Micro)'}"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(df_file, sep=\"\\t\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if names_file is None:\n",
    "    df['group'] = df['filename']\n",
    "    df['label'] = df['filename']\n",
    "    df['is_baseline'] = False\n",
    "else:\n",
    "    # Set method information (optional)\n",
    "    methods = pd.read_csv(names_file, delim_whitespace=True, header=0)\n",
    "    df = pd.merge(df, methods, on='filename', how='left')\n",
    "    df['group'].fillna(df['filename'], inplace=True)\n",
    "    df['label'].fillna(df['filename'], inplace=True)\n",
    "    if 'is_baseline' not in df:\n",
    "        df['is_baseline'] = False\n",
    "    else:\n",
    "        df['is_baseline'].fillna(False, inplace=True)\n",
    "    print(methods)\n",
    "df = df.drop(columns='filename').set_index(['group', 'label', 'ns', 'tau'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign colors based on group\n",
    "cmap = plt.get_cmap('tab20')\n",
    "df['colors'] = df.index.get_level_values('group')\n",
    "df['colors'] = pd.factorize(df['colors'])[0]\n",
    "df['colors'] = df['colors'].apply(lambda x: cmap.colors[x % len(cmap.colors)])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by coverage\n",
    "df = df[df['cov'] >= coverage_threshold]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best methods and thresholds\n",
    "index_best = df.groupby(level=['group', 'ns'])[metric].idxmax() if metric in ['f', 'f_w', 'f_micro', 'f_micro_w'] else df.groupby(['group', 'ns'])[metric].idxmin()\n",
    "index_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe for the best method and threshold\n",
    "df_best = df.loc[index_best, ['cov', 'colors'] + cols + [metric]]\n",
    "df_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe for the best methods\n",
    "df_methods = df.reset_index('tau').loc[[ele[:-1] for ele in index_best], ['tau', 'cov', 'colors'] + cols + [metric]].sort_index()\n",
    "\n",
    "# Makes the curves monotonic. Cumulative max on the last column of the cols variable, e.g. \"pr\" --> precision\n",
    "if cumulate:\n",
    "    if metric in ['f', 'f_w', 'f_micro', 'f_micro_w']:\n",
    "        df_methods[cols[-1]] = df_methods.groupby(level=['label', 'ns'])[cols[-1]].cummax()\n",
    "    else:\n",
    "        df_methods[cols[-1]] = df_methods.groupby(level=['label', 'ns'])[cols[-1]].cummin()\n",
    "\n",
    "\n",
    "# Save to file\n",
    "df_methods.drop(columns=['colors']).to_csv('{}/fig_{}.tsv'.format(out_folder, metric), float_format=\"%.3f\", sep=\"\\t\")\n",
    "df_methods"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Calculate precision-recall AUC\n",
    "# for _, df_g in df_methods.groupby(level=['group', 'label', 'ns']):\n",
    "#     print(df_g[['tau', 'rc', 'pr']])   \n",
    "#     print(df_g['rc'].diff(-1).shift(1))\n",
    "#     print(df_g['pr'])\n",
    "#     print((df_g['rc'].diff(-1).shift(1) * df_g['pr']))\n",
    "#     break"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Calculate average precision score \n",
    "if metric in ['f', 'f_w', 'f_micro', 'f_micro_w']:\n",
    "    df_best['aps'] = df_methods.groupby(level=['group', 'label', 'ns'])[[cols[0], cols[1]]].apply(lambda x: (x[cols[0]].diff(-1).shift(1) * x[cols[1]]).sum())"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Calculate the max coverage across all thresholds\n",
    "df_best['max_cov'] = df_methods.groupby(level=['group', 'label', 'ns'])['cov'].max()\n",
    "df_best"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a label column for the plot legend\n",
    "df_best['label'] = df_best.index.get_level_values('label')\n",
    "if 'aps' not in df_best.columns:\n",
    "    df_best['label'] = df_best.agg(lambda x: f\"{x['label']} ({metric.upper()}={x[metric]:.3f} C={x['max_cov']:.3f})\", axis=1)\n",
    "else:\n",
    "    df_best['label'] = df_best.agg(lambda x: f\"{x['label']} ({metric.upper()}={x[metric]:.3f} APS={x['aps']:.3f} C={x['max_cov']:.3f})\", axis=1)\n",
    "df_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "# F-score contour lines\n",
    "x = np.arange(0.01, 1, 0.01)\n",
    "y = np.arange(0.01, 1, 0.01)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = 2 * X * Y / (X + Y)\n",
    "\n",
    "for ns, df_g in df_best.groupby(level='ns'):\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "     # Contour lines. At the moment they are provided only for the F-score\n",
    "    if metric in ['f', 'f_w', 'f_micro', 'f_micro_w']:\n",
    "        CS = ax.contour(X, Y, Z, np.arange(0.1, 1.0, 0.1), colors='gray')\n",
    "        ax.clabel(CS, inline=True) #, fontsize=10)\n",
    "\n",
    "    # Iterate methods\n",
    "    for i, (index, row) in enumerate(df_g.sort_values(by=[metric, 'max_cov'], ascending=[False if metric in ['f', 'f_w', 'f_micro', 'f_micro_w'] else True, False]).iterrows()):\n",
    "        data = df_methods.loc[index[:-1]]\n",
    "        \n",
    "        # Precision-recall or mi-ru curves\n",
    "        ax.plot(data[cols[0]], data[cols[1]], color=row['colors'], label=row['label'], lw=2, zorder=500-i)\n",
    "        \n",
    "        # F-max or S-min dots\n",
    "        ax.plot(row[cols[0]], row[cols[1]], color=row['colors'], marker='o', markersize=12, mfc='none', zorder=1000-i)\n",
    "        ax.plot(row[cols[0]], row[cols[1]], color=row['colors'], marker='o', markersize=6, zorder=1000-i)\n",
    "\n",
    "    # Set axes limit\n",
    "    if metric in ['f', 'f_w', 'f_micro', 'f_micro_w']:\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(0, 1)\n",
    "   \n",
    "    # plt.xlim(0, max(1, df_best.loc[:,:,ns,:][cols[0]].max()))\n",
    "    # plt.ylim(0, max(1, df_best.loc[:,:,ns,:][cols[1]].max()))\n",
    "\n",
    "    # Set titles\n",
    "    ax.set_title(ns)\n",
    "    ax.set_xlabel(axis_title[cols[0]])\n",
    "    ax.set_ylabel(axis_title[cols[1]])\n",
    "    \n",
    "    # Legend\n",
    "    # ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    leg = ax.legend(markerscale=6)\n",
    "    for legobj in leg.get_lines():\n",
    "        legobj.set_linewidth(10.0)\n",
    "\n",
    "    # Save figure on disk\n",
    "    plt.savefig(\"{}/fig_{}_{}.png\".format(out_folder, metric, ns), bbox_inches='tight')\n",
    "    # plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
